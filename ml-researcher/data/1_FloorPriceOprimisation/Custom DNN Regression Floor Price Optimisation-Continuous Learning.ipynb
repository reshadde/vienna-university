{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DNN Regression: Floor Price Optimisation\n",
    "\n",
    "## Continuous Learning Experiment\n",
    "\n",
    "- Author: Reshad Dernjani\n",
    "- Tensorflow Transform documentation: https://www.tensorflow.org/tfx/transform/tutorials/TFT_census_example\n",
    "- This will load the latest checkpoint of a pre-trained model and continue the training with reduced DSP bids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Usage\n",
    "- Make sure transformed data was made available by running \"Preprocessing with Tensorflow Transform-Continuous Learning.ipynb\"\n",
    "- Make sure base model was created by running \"Custom DNN Regression Floor Price Optimisation.ipynb\"\n",
    "- Make sure CHECKPOINT is set correctly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import argparse\n",
    "import os\n",
    "import pprint\n",
    "import tempfile\n",
    "import time\n",
    "\n",
    "try:\n",
    "    import tensorflow_transform as tft\n",
    "    import apache_beam as beam\n",
    "    import tensorflow_model_analysis as tfma\n",
    "except ImportError:\n",
    "    # This will take a minute, ignore the warnings.\n",
    "    !pip install -q tensorflow-transform\n",
    "    !pip install -q apache_beam\n",
    "    !pip install -q tensorflow-model-analysis\n",
    "    import tensorflow_transform as tft\n",
    "    import apache_beam as beam\n",
    "    import tensorflow_model_analysis as tfma\n",
    "\n",
    "# This will seed the reandom generators the same way, in order\n",
    "# to make results more deterministic. Neural networks use ramdomness by design \n",
    "# for weights initialization, regularization, word embedding or stochastic optimizers.\n",
    "from numpy.random import seed\n",
    "seed(1)\n",
    "from tensorflow import set_random_seed\n",
    "set_random_seed(2)    \n",
    "    \n",
    "import tensorflow as tf\n",
    "import tensorflow_transform.beam as tft_beam\n",
    "from tensorflow_transform.tf_metadata import dataset_metadata\n",
    "from tensorflow_transform.tf_metadata import dataset_schema\n",
    "\n",
    "from tensorflow_transform.saved import saved_transform_io\n",
    "from tensorflow_transform.beam.tft_beam_io import transform_fn_io"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting hyperparameters and basic housekeeping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CHECKPOINT = '/notebooks/tmp/47/logs/model.ckpt-511'\n",
    "\n",
    "TRAIN_NUM_EPOCHS = 1\n",
    "TRAIN_BATCH_SIZE = 1\n",
    "HIDDEN_UNITS = [1024]\n",
    "LEARNING_RATE = 0.001\n",
    "SHIFT_LOSS = 0.7\n",
    "\n",
    "IS_TESTING = False\n",
    "NUM_TEST_INSTANCES = 1000\n",
    "                \n",
    "# Working directories\n",
    "TEMP = '/notebooks/tmp/'\n",
    "TRANSFORMED_DIR = '/notebooks/transformed/'\n",
    "\n",
    "# Names of temp files\n",
    "TRANSFORMED_TRAIN_DATA_FILEBASE = 'train_transformed'\n",
    "TRANSFORMED_TEST_DATA_FILEBASE = 'test_transformed'\n",
    "EXPORTED_MODEL_DIR = 'exported_model_dir'\n",
    "EXPORTED_EVAL_MODEL_DIR = 'eval_dir'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Name our columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CATEGORICAL_FEATURE_KEYS = [\n",
    "    'inventory_id',\n",
    "    'request_type',\n",
    "    'state_code',\n",
    "    'country_code',\n",
    "    'city_code',\n",
    "    'device_os',\n",
    "    'device_os_version',\n",
    "    'hour_of_day',\n",
    "]\n",
    "\n",
    "NUMERIC_FEATURE_KEYS = [\n",
    "    'ex_floor_price',\n",
    "]\n",
    "\n",
    "OPTIONAL_NUMERIC_FEATURE_KEYS = [ \n",
    "    # Actually we handled optionals on the data query (at least for research).\n",
    "]\n",
    "\n",
    "LABEL_KEY = 'ex_bid_price'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define our features and schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RAW_DATA_FEATURE_SPEC = dict(\n",
    "    [(name, tf.FixedLenFeature([], tf.string))\n",
    "     for name in CATEGORICAL_FEATURE_KEYS] +\n",
    "    [(name, tf.FixedLenFeature([], tf.float32))\n",
    "     for name in NUMERIC_FEATURE_KEYS] +\n",
    "    [(name, tf.VarLenFeature(tf.float32))\n",
    "     for name in OPTIONAL_NUMERIC_FEATURE_KEYS] +\n",
    "    [(LABEL_KEY, tf.FixedLenFeature([], tf.float32))]\n",
    ")\n",
    "\n",
    "RAW_DATA_METADATA = dataset_metadata.DatasetMetadata(\n",
    "    dataset_schema.from_feature_spec(RAW_DATA_FEATURE_SPEC)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create an input function for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _make_training_input_fn(tf_transform_output, transformed_examples, batch_size):\n",
    "    \"\"\"Creates an input function reading from transformed data.\n",
    "    \n",
    "    Args:\n",
    "        tf_transform_output: Wrapper around output of tf.Transform.\n",
    "        transformed_examples: Base filename of examples.\n",
    "        batch_size: Batch size.\n",
    "        \n",
    "    Returns:\n",
    "        The input function for training or eval.\n",
    "    \"\"\"\n",
    "    def input_fn():\n",
    "        \"\"\"Input function for training and eval.\"\"\"\n",
    "        dataset = tf.data.experimental.make_batched_features_dataset(\n",
    "            file_pattern=transformed_examples,\n",
    "            batch_size=batch_size,\n",
    "            features=tf_transform_output.transformed_feature_spec(),\n",
    "            reader=tf.data.TFRecordDataset,\n",
    "            shuffle=True\n",
    "        )\n",
    "\n",
    "        transformed_features = dataset.make_one_shot_iterator().get_next()\n",
    "        # Extract features and label(s) from the transformed tensors.\n",
    "        transformed_labels = transformed_features.pop(LABEL_KEY)\n",
    "\n",
    "        return transformed_features, transformed_labels\n",
    "\n",
    "    return input_fn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create an input function for serving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _make_serving_input_fn(tf_transform_output):\n",
    "    \"\"\"Creates an input function reading from raw data.\n",
    "    \n",
    "    Args:\n",
    "        tf_transform_output: Wrapper around output of tf.Transform.\n",
    "        \n",
    "    Returns:\n",
    "        The serving input function.\n",
    "    \"\"\"\n",
    "    raw_feature_spec = RAW_DATA_METADATA.schema.as_feature_spec()\n",
    "    # Remove label since it is not available during serving.\n",
    "    raw_feature_spec.pop(LABEL_KEY)\n",
    "\n",
    "    def serving_input_fn():\n",
    "        \"\"\"Input function for serving.\"\"\"\n",
    "        # Get raw features by generating the basic serving input_fn and calling it.\n",
    "        # Here we generate an input_fn that expects a parsed data point to be fed to the model at serving time.\n",
    "        # See also: tf.estimator.export.build_raw_serving_input_receiver_fn.\n",
    "        raw_input_fn = tf.estimator.export.build_parsing_serving_input_receiver_fn(\n",
    "            raw_feature_spec, \n",
    "            default_batch_size=None\n",
    "        )\n",
    "        serving_input_receiver = raw_input_fn()\n",
    "    \n",
    "        # Apply the transform function that was used to generate the materialized data.\n",
    "        raw_features = serving_input_receiver.features\n",
    "        transformed_features = tf_transform_output.transform_raw_features(raw_features)\n",
    "\n",
    "        return tf.estimator.export.ServingInputReceiver(raw_features, serving_input_receiver.receiver_tensors)\n",
    "\n",
    "    return serving_input_fn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wrap our input data in FeatureColumns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_feature_columns(tf_transform_output):\n",
    "    \"\"\"Returns the FeatureColumns for the model.\n",
    "  \n",
    "    Args:\n",
    "        tf_transform_output: A `TFTransformOutput` object.\n",
    "      \n",
    "    Returns:\n",
    "        A list of FeatureColumns.\n",
    "    \"\"\"\n",
    "    # Wrap scalars as real value columns.\n",
    "    real_value_columns = [\n",
    "        tf.feature_column.numeric_column(key, shape=())\n",
    "        for key in NUMERIC_FEATURE_KEYS\n",
    "    ]\n",
    "\n",
    "    # Wrap categorical columns.\n",
    "    one_hot_encoded_columns = [\n",
    "        tf.feature_column.categorical_column_with_vocabulary_file(\n",
    "            key=key,\n",
    "            vocabulary_file=tf_transform_output.vocabulary_file_by_name(vocab_filename=key)\n",
    "        )\n",
    "        for key in CATEGORICAL_FEATURE_KEYS\n",
    "    ]\n",
    "  \n",
    "    # Wrap indicator colmuns and use embedding on high dimensional feature columns.\n",
    "    embedding_columns = []\n",
    "    indicator_columns = []\n",
    "    for column in one_hot_encoded_columns:\n",
    "        if(column.key == 'hour_of_day'):\n",
    "            indicator_columns.append(tf.feature_column.indicator_column(column))\n",
    "        elif(column.key == 'device_os_version'):\n",
    "            indicator_columns.append(tf.feature_column.indicator_column(column))\n",
    "        elif(column.key == 'device_os'):\n",
    "            indicator_columns.append(tf.feature_column.indicator_column(column))\n",
    "        elif(column.key == 'city_code'):\n",
    "            vocab_size = tf_transform_output.vocabulary_size_by_name('city_code')\n",
    "            embedding_columns.append(tf.feature_column.embedding_column(column, round(vocab_size**0.25, 0)))\n",
    "        elif(column.key == 'country_code'):\n",
    "            indicator_columns.append(tf.feature_column.indicator_column(column))\n",
    "        elif(column.key == 'state_code'):\n",
    "            vocab_size = tf_transform_output.vocabulary_size_by_name('state_code')\n",
    "            embedding_columns.append(tf.feature_column.embedding_column(column, round(vocab_size**0.25, 0)))\n",
    "        elif(column.key == 'request_type'):\n",
    "            indicator_columns.append(tf.feature_column.indicator_column(column))\n",
    "        elif(column.key == 'inventory_id'):\n",
    "            vocab_size = tf_transform_output.vocabulary_size_by_name('inventory_id')\n",
    "            embedding_columns.append(tf.feature_column.embedding_column(column, round(vocab_size**0.25, 0)))\n",
    "    \n",
    "    return real_value_columns + indicator_columns + embedding_columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def floorPriceEstimator(features, labels, mode, params):\n",
    "    \"\"\"This is a custom Neural Network Regressor implementation,\n",
    "    which uses a asymmetric loss function.\n",
    "    \n",
    "    Args:\n",
    "        features: Feature columns.\n",
    "        labels: Labels columns.\n",
    "        mode: predict, train or eval mode.\n",
    "        params: Configuration dict containing keys:\n",
    "            feature_columns, hidden_units, learning_rate, optimizer or shift_loss.\n",
    "        \n",
    "    Returns:\n",
    "        EstimatorSpec: Objects of EstimatorSpec define the model to be run.\n",
    "        \n",
    "    \"\"\"\n",
    "    \n",
    "    # Using 'input_layer' to apply the feature columns.\n",
    "    net = tf.feature_column.input_layer(features, params['feature_columns'])\n",
    "\n",
    "    # Build the hidden layers, sized according to the 'hidden_units' param.\n",
    "    for units in params['hidden_units']:\n",
    "        net = tf.layers.dense(net, units=units, activation=tf.nn.relu)\n",
    "\n",
    "    # Output layer takes as input the results of the latest hidden layer\n",
    "    net = tf.layers.dense(net, units=1, activation=None)\n",
    "    \n",
    "    # Reshape the output layer to a 1-dim Tensor to return predictions\n",
    "    net = tf.squeeze(net, 1)\n",
    "\n",
    "    if mode == tf.estimator.ModeKeys.PREDICT:\n",
    "        return tf.estimator.EstimatorSpec(\n",
    "            mode=mode, \n",
    "            predictions={\"predictions\": net}\n",
    "        )\n",
    "    # Get batch size\n",
    "    batch_size = tf.shape(labels)[0]\n",
    "    \n",
    "    # Tweaked squared loss function\n",
    "    # Positive values for shift_loss penalize overestimation. -1 < shift_loss < 1\n",
    "    def asymmetric_loss(shift_loss): \n",
    "        return tf.pow(net-labels, 2) * tf.pow(tf.sign(net-labels) + shift_loss, 2)\n",
    "    \n",
    "    # Calculate asymmetric loss function\n",
    "    asymmetric_loss = tf.reduce_sum(asymmetric_loss(params['shift_loss']))\n",
    "    # Push metric to logs\n",
    "    tf.summary.scalar(\"asymmetric_loss\", asymmetric_loss)\n",
    "    \n",
    "    # Calculate mean squared error.\n",
    "    mse = tf.metrics.mean_squared_error(tf.cast(labels, tf.float32), net)\n",
    "    total_mse = tf.to_float(batch_size) * mse[1]\n",
    "    # Push metric to logs\n",
    "    tf.summary.scalar('total_mean_squared_error', total_mse)\n",
    "    \n",
    "    total_loss = tf.to_float(batch_size) * asymmetric_loss\n",
    "\n",
    "    if mode == tf.estimator.ModeKeys.TRAIN:\n",
    "        optimizer = params.get(\"optimizer\", tf.train.AdamOptimizer)\n",
    "        optimizer = optimizer(params.get(\"learning_rate\", None))\n",
    "        train_op = optimizer.minimize(loss=asymmetric_loss, global_step=tf.train.get_global_step())\n",
    "\n",
    "        return tf.estimator.EstimatorSpec(\n",
    "            mode=mode, \n",
    "            loss=total_loss, \n",
    "            train_op=train_op\n",
    "        )\n",
    "\n",
    "    assert mode == tf.estimator.ModeKeys.EVAL\n",
    "    \n",
    "    # Calculate root mean squared error\n",
    "    rmse = tf.metrics.root_mean_squared_error(tf.cast(labels, tf.float32), net)\n",
    "        \n",
    "    def asymmetric_metric_fn(predictions=[], labels=[]):\n",
    "        R, update_op1 = tf.contrib.metrics.streaming_recall(predictions, labels)\n",
    "        R = asymmetric_loss\n",
    "        return R, update_op1\n",
    "    \n",
    "    eval_metrics = {\n",
    "        \"mean_squared_error\": mse,\n",
    "        \"root_mean_squared_error\": rmse,\n",
    "        \"asymmetric_loss\": asymmetric_metric_fn(\n",
    "            predictions=net, \n",
    "            labels=labels\n",
    "        )\n",
    "    }\n",
    "\n",
    "    return tf.estimator.EstimatorSpec(\n",
    "        mode=mode,\n",
    "        loss=total_loss,\n",
    "        eval_metric_ops=eval_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train, Evaluate, and Export our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_evaluate(working_dir, transformed_dir, num_train_instances,\n",
    "                       num_test_instances=NUM_TEST_INSTANCES):\n",
    "    \"\"\"Train the model on training data and evaluate on test data.\n",
    "  \n",
    "    Args:\n",
    "        transformed_dir: Directory to read transformed data and metadata from\n",
    "        num_train_instances: Number of instances in train set\n",
    "        num_test_instances: Number of instances in test set\n",
    "    \n",
    "    Returns:\n",
    "        The results from the estimator's 'evaluate' method\n",
    "    \"\"\"\n",
    "    # Get transformed data\n",
    "    tf_transform_output = tft.TFTransformOutput(transformed_dir)\n",
    "\n",
    "    # This will load the model trained with 24 hours,\n",
    "    # in order to warm start the training with the reduced bids data of the next day hour 09\n",
    "    ws = tf.estimator.WarmStartSettings(ckpt_to_initialize_from=CHECKPOINT)\n",
    "    \n",
    "    # Init estimator.\n",
    "    estimator = tf.estimator.Estimator(\n",
    "        model_fn=floorPriceEstimator,\n",
    "        model_dir=working_dir + '/logs',\n",
    "        warm_start_from=ws,\n",
    "        params={\n",
    "            'feature_columns': get_feature_columns(tf_transform_output),\n",
    "            'hidden_units': HIDDEN_UNITS,\n",
    "            'learning_rate': LEARNING_RATE,\n",
    "            'optimizer': tf.train.AdamOptimizer,\n",
    "            'shift_loss': SHIFT_LOSS\n",
    "        }\n",
    "    )\n",
    "\n",
    "    # Init TRAIN input function\n",
    "    train_input_fn = _make_training_input_fn(\n",
    "        tf_transform_output,\n",
    "        os.path.join(transformed_dir, TRANSFORMED_TRAIN_DATA_FILEBASE + '*'),\n",
    "        batch_size=TRAIN_BATCH_SIZE\n",
    "    )\n",
    "\n",
    "    # Fit the model\n",
    "    train_metrics = estimator.train(\n",
    "        input_fn=train_input_fn, \n",
    "        steps=TRAIN_NUM_EPOCHS * num_train_instances / TRAIN_BATCH_SIZE\n",
    "    )\n",
    "    \n",
    "    # Export the trained model.\n",
    "    serving_input_fn = _make_serving_input_fn(tf_transform_output)\n",
    "    exported_model_dir = os.path.join(working_dir, EXPORTED_MODEL_DIR)\n",
    "    estimator.export_savedmodel(exported_model_dir, serving_input_fn)\n",
    "        \n",
    "    if IS_TESTING:\n",
    "        # Init TRAIN input function\n",
    "        eval_train_input_fn = _make_training_input_fn(\n",
    "            tf_transform_output,\n",
    "            os.path.join(transformed_dir, TRANSFORMED_TRAIN_DATA_FILEBASE + '*'),\n",
    "            batch_size=1\n",
    "        )\n",
    "    \n",
    "        # Eval the model on TRAIN.\n",
    "        eval_metrics = estimator.evaluate(input_fn=eval_train_input_fn, steps=num_test_instances)\n",
    "        print('\\n\\nEval metrics on TRAIN')\n",
    "        pprint.pprint(eval_metrics)\n",
    "    \n",
    "        # Init TEST input function\n",
    "        eval_test_input_fn = _make_training_input_fn(\n",
    "            tf_transform_output,\n",
    "            os.path.join(transformed_dir, TRANSFORMED_TEST_DATA_FILEBASE + '*'),\n",
    "            batch_size=1\n",
    "        )\n",
    "    \n",
    "        # Eval the model on TEST .\n",
    "        eval_metrics = estimator.evaluate(input_fn=eval_test_input_fn, steps=num_test_instances)\n",
    "        print('\\n\\nEval metrics on TEST')\n",
    "        pprint.pprint(eval_metrics)\n",
    "    \n",
    "        # Run predictions on the model.\n",
    "        estimator.predict(input_fn=eval_test_input_fn)\n",
    "        \n",
    "    return estimator\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_instances(tf_records_filenames):\n",
    "    counter = 0\n",
    "    for fn in tf_records_filenames:\n",
    "        for record in tf.python_io.tf_record_iterator(fn):\n",
    "            counter += 1 \n",
    "    return counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_hyperparameters(dsp, num_train_instances):\n",
    "    if not os.path.exists(TEMP+dsp):\n",
    "        os.makedirs(TEMP+dsp)\n",
    "    with open(TEMP+dsp+\"/hyperparameters.txt\", \"a+\") as text_file:\n",
    "        print(\"\\nTRAIN_NUM_EPOCHS: {}\".format(TRAIN_NUM_EPOCHS), file=text_file)\n",
    "        print(\"TRAIN_BATCH_SIZE: {}\".format(TRAIN_BATCH_SIZE), file=text_file)\n",
    "        print(\"LEARNING_RATE: {}\".format(LEARNING_RATE), file=text_file)\n",
    "        print(\"SHIFT_LOSS: {}\".format(SHIFT_LOSS), file=text_file)\n",
    "        for unit in HIDDEN_UNITS:\n",
    "            print(\"HIDDEN LAYER{}: {}\".format(HIDDEN_UNITS.index(unit), unit), file=text_file)\n",
    "        print(\"NUMBER_OF_TRAIN_INSTANCES: {}\".format(num_train_instances), file=text_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_hyperparameters(dsp, num_train_instances):\n",
    "    print(\"\\nTRAIN_NUM_EPOCHS: {}\".format(TRAIN_NUM_EPOCHS))\n",
    "    print(\"TRAIN_BATCH_SIZE: {}\".format(TRAIN_BATCH_SIZE))\n",
    "    print(\"LEARNING_RATE: {}\".format(LEARNING_RATE))\n",
    "    print(\"SHIFT_LOSS: {}\".format(SHIFT_LOSS))\n",
    "    for unit in HIDDEN_UNITS:\n",
    "        print(\"HIDDEN LAYER{}: {}\".format(HIDDEN_UNITS.index(unit), unit))\n",
    "    print(\"NUMBER_OF_TRAIN_INSTANCES: {}\".format(num_train_instances))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Put it all together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Remove following line to see more details during training\n",
    "#tf.logging.set_verbosity(tf.logging.ERROR)\n",
    "\n",
    "dsp_list = ['47_warmstarting']\n",
    "\n",
    "for dsp in dsp_list:\n",
    "    start = time.time()\n",
    "    tf_records_filenames = tf.gfile.Glob(TRANSFORMED_DIR + dsp + '/' + TRANSFORMED_TRAIN_DATA_FILEBASE + '*')\n",
    "    num_train_instances = count_instances(tf_records_filenames)\n",
    "    save_hyperparameters(dsp, num_train_instances)\n",
    "        \n",
    "    estimator = train_and_evaluate(\n",
    "        working_dir=TEMP+dsp, \n",
    "        transformed_dir=TRANSFORMED_DIR + dsp, \n",
    "        num_train_instances= num_train_instances\n",
    "    )\n",
    "    print_hyperparameters(dsp, num_train_instances)\n",
    "    print('\\n\\nTraining for dsp {} took {:.2f} seconds'.format(dsp, time.time() - start))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
