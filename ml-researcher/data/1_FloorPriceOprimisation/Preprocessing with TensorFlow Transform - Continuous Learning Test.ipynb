{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Transformation: Floor Price Optimisation\n",
    "\n",
    "## Continuous Learning Experiment\n",
    "\n",
    "- Author: Reshad Dernjani\n",
    "- Source: Tensorflow Transform documentation https://www.tensorflow.org/tfx/transform/tutorials/TFT_census_example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Usage\n",
    "Provide your raw data the following way:\n",
    "* 8% unoptimized traffic for 24 hours \n",
    " * /notebooks/raw_data/[dsp_id]/dsp-[dsp_id]-train.csv\n",
    "* 8% unoptimized traffic of the next day on certain hours\n",
    " * Next day hour 9 reduced dsp bids by 90%. Will be used for warm start training\n",
    "   * /notebooks/raw_data/[dsp_id]/dsp-[dsp_id]-test-9-reduced.csv\n",
    " * Next day hour 10 reduced dsp bids by 90%. Will be used for the analysis\n",
    "   * /notebooks/raw_data/[dsp_id]/dsp-[dsp_id]-test-10-reduced.csv\n",
    " \n",
    "- Replace [dsp_id] with the actual dsp id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import argparse\n",
    "import os\n",
    "import pprint\n",
    "import tempfile\n",
    "import time\n",
    "\n",
    "try:\n",
    "    import tensorflow_transform as tft\n",
    "    import apache_beam as beam\n",
    "except ImportError:\n",
    "    # This will take a minute, ignore the warnings.\n",
    "    !pip install -q tensorflow-transform\n",
    "    !pip install -q apache_beam\n",
    "    import tensorflow_transform as tft\n",
    "    import apache_beam as beam\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_transform.beam as tft_beam\n",
    "from tensorflow_transform.tf_metadata import dataset_metadata\n",
    "from tensorflow_transform.tf_metadata import dataset_schema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic housekeeping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_PATH = '/notebooks/transformed'\n",
    "RAW_PATH = '/notebooks/raw_data'\n",
    "# Output name templates.\n",
    "TRANSFORMED_TRAIN_DATA_FILEBASE = 'train_transformed'\n",
    "TRANSFORMED_TEST_DATA_FILEBASE = 'test_transformed'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Name our columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Order of columns in the csv.\n",
    "ORDERED_COLUMNS = [\n",
    "    \"inventory_id\",\n",
    "    \"request_type\",\n",
    "    \"ex_floor_price\",\n",
    "    \"ex_bid_price\",\n",
    "    \"state_code\",\n",
    "    \"country_code\",\n",
    "    \"city_code\",\n",
    "    \"device_os\",\n",
    "    \"device_os_version\",\n",
    "    \"hour_of_day\"\n",
    "]\n",
    "\n",
    "CATEGORICAL_FEATURE_KEYS = [\n",
    "    'inventory_id',\n",
    "    'request_type',\n",
    "    'state_code',\n",
    "    'country_code',\n",
    "    'city_code',\n",
    "    'device_os',\n",
    "    'device_os_version',\n",
    "    'hour_of_day',\n",
    "]\n",
    "\n",
    "NUMERIC_FEATURE_KEYS = [\n",
    "    'ex_floor_price',\n",
    "]\n",
    "\n",
    "OPTIONAL_NUMERIC_FEATURE_KEYS = [ \n",
    "    # Actually we handled optionals on the data query (at least during research).\n",
    "]\n",
    "\n",
    "LABEL_KEY = 'ex_bid_price'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define our features and schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RAW_DATA_FEATURE_SPEC = dict(\n",
    "    [(name, tf.FixedLenFeature([], tf.string))\n",
    "     for name in CATEGORICAL_FEATURE_KEYS] +\n",
    "    [(name, tf.FixedLenFeature([], tf.float32))\n",
    "     for name in NUMERIC_FEATURE_KEYS] +\n",
    "    [(name, tf.VarLenFeature(tf.float32))\n",
    "     for name in OPTIONAL_NUMERIC_FEATURE_KEYS] +\n",
    "    [(LABEL_KEY, tf.FixedLenFeature([], tf.float32))]\n",
    ")\n",
    "\n",
    "RAW_DATA_METADATA = dataset_metadata.DatasetMetadata(\n",
    "    dataset_schema.from_feature_spec(RAW_DATA_FEATURE_SPEC)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a Beam Transform for cleaning our input data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MapAndFilterErrors(beam.PTransform):\n",
    "    \"\"\"Like beam.Map but filters out erros in the map_fn.\"\"\"\n",
    "\n",
    "    class _MapAndFilterErrorsDoFn(beam.DoFn):\n",
    "        \"\"\"Count the bad examples using a beam metric.\"\"\"\n",
    "\n",
    "        def __init__(self, fn):\n",
    "            self._fn = fn\n",
    "            # Create a counter to measure number of bad elements.\n",
    "            self._bad_elements_counter = beam.metrics.Metrics.counter('floor_price_optimisation', 'bad_elements')\n",
    "\n",
    "        def process(self, element):\n",
    "            try:\n",
    "                yield self._fn(element)\n",
    "            except Exception:\n",
    "                # Catch any exception of the above call.\n",
    "                self._bad_elements_counter.inc(1)\n",
    "\n",
    "    def __init__(self, fn):\n",
    "        self._fn = fn\n",
    "\n",
    "    def expand(self, pcoll):\n",
    "        return pcoll | beam.ParDo(self._MapAndFilterErrorsDoFn(self._fn))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a tf.Transform preprocessing_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing_fn(inputs):\n",
    "    \"\"\"Preprocess input columns into transformed columns.\"\"\"\n",
    "    # Since we are modifying some features and leaving others unchanged, we\n",
    "    # start by setting 'outputs' to a copy of 'inputs'.\n",
    "    outputs = inputs.copy()\n",
    "\n",
    "    # Scale numeric columns to have range [0, 1].\n",
    "    for key in NUMERIC_FEATURE_KEYS:\n",
    "        outputs[key] = tft.scale_to_0_1(outputs[key])\n",
    "\n",
    "    # For all categorical columns except the label column, we generate a\n",
    "    # vocabulary but do not modify the feature.  This vocabulary is instead\n",
    "    # used in the trainer, by means of a feature column, to convert the feature\n",
    "    # from a string to an integer id.\n",
    "    for key in CATEGORICAL_FEATURE_KEYS:\n",
    "        tft.vocabulary(inputs[key], vocab_filename=key)\n",
    "    \n",
    "    for key in OPTIONAL_NUMERIC_FEATURE_KEYS:\n",
    "        # This is a SparseTensor because it is optional. \n",
    "        # Here we fill in a default value when it is missing.\n",
    "        sparse_tensor = tf.SparseTensor(outputs[key].indices, outputs[key].values, [outputs[key].dense_shape[0], 1])\n",
    "        dense = tf.sparse.to_dense(\n",
    "            sparse_tensor,\n",
    "            default_value=0.,\n",
    "            validate_indices=True,\n",
    "            name=None\n",
    "        )\n",
    "        # Reshaping from a batch of vectors of size 1 to a batch of scalars.\n",
    "        dense = tf.squeeze(dense, axis=1)\n",
    "        outputs[key] = tft.scale_to_0_1(dense)\n",
    "        \n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transform the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_data(train_data_file, test_data_files, working_dir, \n",
    "                   transformed_train_file_suffix='', transformed_test_file_suffix=''):\n",
    "    \"\"\"Transform the data and write out as a TFRecord.\n",
    "\n",
    "    Read in the data using the CSV reader, and transform it using a\n",
    "    preprocessing pipeline that scales numeric data and converts categorical data\n",
    "    from strings to int64 value indices, by creating a vocabulary for each category.\n",
    "\n",
    "    Args:\n",
    "        train_data_files: Files containing training data\n",
    "        test_data_files: Files containing test data\n",
    "        working_dir: Directory to write transformed data and metadata to\n",
    "        transformed_train_file_suffix: Suffix to be appended to the tranformed files\n",
    "        transformed_test_file_suffix: Suffix to be appended to the tranformed files\n",
    "    \"\"\"\n",
    "\n",
    "    # The \"with\" block will create a pipeline, and run that \n",
    "    # pipeline at the exit of the block.\n",
    "    with beam.Pipeline() as pipeline:\n",
    "        with tft_beam.Context(temp_dir=tempfile.mkdtemp()):\n",
    "            # Create a coder to read the data with the schema.  To do this we\n",
    "            # need to list all columns in order since the schema doesn't specify the\n",
    "            # order of columns in the csv.\n",
    "            converter = tft.coders.CsvCoder(ORDERED_COLUMNS, RAW_DATA_METADATA.schema)\n",
    "                \n",
    "            # Read in raw data and convert using CSV converter. \n",
    "            # We use MapAndFilterErrors instead of Map to filter out decode errors in\n",
    "            # convert.decode, which anyway should not occure since we are in control of the data generation.\n",
    "            raw_data = (\n",
    "                pipeline\n",
    "                | 'ReadTrainData' >> beam.io.ReadFromText(train_data_file,skip_header_lines=1)\n",
    "                | 'DecodeTrainData' >> MapAndFilterErrors(converter.decode))\n",
    "\n",
    "            # Combine data and schema into a dataset tuple.  Note that we already used\n",
    "            # the schema to read the CSV data, but we also need it to interpret raw_data.\n",
    "            raw_dataset = (raw_data, RAW_DATA_METADATA)\n",
    "            transformed_dataset, transform_fn = (raw_dataset | tft_beam.AnalyzeAndTransformDataset(preprocessing_fn))\n",
    "            transformed_data, transformed_metadata = transformed_dataset\n",
    "            transformed_data_coder = tft.coders.ExampleProtoCoder(transformed_metadata.schema)\n",
    "        \n",
    "            _ = (\n",
    "                transformed_data\n",
    "                | 'EncodeTrainData' >> beam.Map(transformed_data_coder.encode)\n",
    "                | 'WriteTrainData' >> beam.io.WriteToTFRecord(\n",
    "                    os.path.join(working_dir, \n",
    "                        TRANSFORMED_TRAIN_DATA_FILEBASE+transformed_train_file_suffix)))\n",
    "            \n",
    "            j = 0\n",
    "            for test_file in test_data_files:\n",
    "                # Get suffix of raw data file e.g 'dsp-47-test-10hours.csv' --> ' -10hours'\n",
    "                begin = test_file.find(\"test\") + 4\n",
    "                end = len(test_file) - 4\n",
    "                raw_test_file_suffix = test_file[begin:end]\n",
    "                \n",
    "                # Now apply transform function to test data.\n",
    "                raw_test_data = (\n",
    "                    pipeline\n",
    "                    | 'ReadTestData'+str(j) >> beam.io.ReadFromText(test_file, skip_header_lines=1)\n",
    "                    | 'DecodeTestData'+str(j) >> MapAndFilterErrors(converter.decode))\n",
    "\n",
    "                raw_test_dataset = (raw_test_data, RAW_DATA_METADATA)\n",
    "                transformed_test_dataset = ((raw_test_dataset, transform_fn) | \"test\"+str(j) >> tft_beam.TransformDataset())\n",
    "                # Don't need transformed data schema, since it's the same as before.\n",
    "                transformed_test_data, _ = transformed_test_dataset\n",
    "\n",
    "                _ = (\n",
    "                    transformed_test_data\n",
    "                    | 'EncodeTestData'+str(j) >> beam.Map(transformed_data_coder.encode)\n",
    "                    | 'WriteTestData'+str(j) >> beam.io.WriteToTFRecord(\n",
    "                        os.path.join(working_dir, \n",
    "                            TRANSFORMED_TEST_DATA_FILEBASE+raw_test_file_suffix+transformed_test_file_suffix)))\n",
    "                j += 1\n",
    "                \n",
    "            # Will write a SavedModel and metadata to working_dir, which can then\n",
    "            # be read by the tft.TFTransformOutput class.\n",
    "            _ = (\n",
    "                transform_fn\n",
    "                | 'WriteTransformFn' >> tft_beam.WriteTransformFn(working_dir))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Put it all together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# NOTE: After transforming the raw data we would need \n",
    "# to delete the file \"transfomred/train_transformed-00000-of-00001\" \n",
    "# and rename \"transfomred/test_transformed-9-reduced-00000-of-00001\" \n",
    "# to \"transfomred/train_transformed-9-reduced-00000-of-00001\" in order to continue with the training!\n",
    "\n",
    "dsps = ['47_warmstarting']\n",
    "for dsp in dsps:\n",
    "    final_output_path = OUTPUT_PATH+'/'+dsp\n",
    "    test_data_paths = tf.gfile.Glob(RAW_PATH+'/'+dsp+'/dsp-'+dsp+'-test*')\n",
    "    train_data_path = tf.gfile.Glob(RAW_PATH+'/'+dsp+'/dsp-'+dsp+'-train*')[0]\n",
    "    start = time.time() \n",
    "    transform_data(train_data_path, test_data_paths, final_output_path,\n",
    "                   transformed_train_file_suffix='', transformed_test_file_suffix='')\n",
    "    print('Transform took {:.2f} seconds for DSP {}'.format(time.time() - start, dsp))\n",
    "    \n",
    "# Ignore\"WARNING:tensorflow:Expected binary or unicode string, got type_url:\",\n",
    "# since it is a known bug and will be fixed soon. \n",
    "# See: https://stackoverflow.com/questions/49394549/error-on-data-type-from-trainable-variables\n",
    "# The same goes for \"WARNING:root:Couldn't find python-snappy so the implementation of _TFRecordUtil._masked_crc32c is not as fast as it could be.\" \n",
    "# See: https://github.com/apache/beam/pull/6801 \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
